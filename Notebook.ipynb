{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "0ewKYZrLLmKG",
        "bdTP17X3Spkp",
        "ov-9TAm7Stnk",
        "EoEFur5ES4zz",
        "C4d-JEmQTGdT",
        "d70EA78STKYR",
        "sg4r5f20gYi2"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi7QfD61CJc_"
      },
      "source": [
        "## Przygotowanie\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8tSbHLQ0JoN"
      },
      "source": [
        "Rozpoczniemy Od przygotowania środowiska - jeśli używasz notatnika w środowisku Colab - aktualizacja biblioteki \"statsmodels\" do wersji 0.12. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EsL1C8C9Bmj"
      },
      "source": [
        "!pip install statsmodels --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVd8ptoFJZw5"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import statsmodels.api as sm\n",
        "import itertools\n",
        "\n",
        "keras = tf.keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS4KvziC0bb0"
      },
      "source": [
        "Następnie zdefiniujemy funkcje pomocnicze odpowiadające zarówno za tworzenie wykresów oraz za przetworzenie danych"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeEbf8l9J6TN"
      },
      "source": [
        "def plot_series(time, series, format=\"-\", start=0, end=None, label=None):\n",
        "    plt.plot(time[start:end], series[start:end], format, label=label)\n",
        "    ax = plt.gca()\n",
        "    ax.xaxis.set_major_locator(plt.MaxNLocator(8))\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    if label:\n",
        "        plt.legend(fontsize=14)\n",
        "    plt.grid(True)\n",
        "\n",
        "def print_error(valid, predicted): \n",
        "    print('mean absoulte error:')\n",
        "    print(keras.metrics.mean_absolute_error(valid, predicted).numpy())\n",
        "    print('mean squared error')\n",
        "    print(keras.metrics.mean_squared_error(valid, predicted).numpy())  \n",
        "\n",
        "def sequential_window_dataset(series, window_size):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=window_size, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda window: window.batch(window_size + 1))\n",
        "    ds = ds.map(lambda window: (window[:-1], window[1:]))\n",
        "    return ds.batch(1).prefetch(1)\n",
        "  \n",
        "def seq2seq_window_dataset(series, window_size, batch_size=32,\n",
        "                           shuffle_buffer=1000):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs):\n",
        "        self.model.reset_states()\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjKYzZIDFvsz"
      },
      "source": [
        "#Szereg czasowy\n",
        "Szeregiem czasowym nazywamy uporządkowaną sekwencję, w której dane ułożone są sekwencyjnie, a kluczem określającym ich położenie jest czas.\n",
        "W zależności od wartości określanych przez encjęw danym punkcie w czasie, dane możemy podzielić na:\n",
        "- jednowymiarowe (jedna wartość w jednym punkcie)\n",
        "- wielowymiarowe (wiele wartości w jednym punkcie)\n",
        "\n",
        "#Występowanie\n",
        "Szeregi czasowe występują dosłownie w każdej dziedzinie życia. Ich zastosowanie można łatwo dostrzec m.in w prognozach pogody, na giełdzie, w historycznych danych, pomiarach.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ewKYZrLLmKG"
      },
      "source": [
        "#Przykładowy zbiór danych jednowymiarowych.\n",
        "W tym notatniku pokażemy metody predykcji szeregów czasowych na przykładzie szeregów jednowymiarowych. Przykładowy szereg przedstawiający ilość sprzedanych szamponów na przestrzeni 3 lat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgw8e11xHs2d"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/Mervolt/TimeSeriesTutorial/master/shampoo.csv\"\n",
        "\n",
        "shampoo_dataset = pd.read_csv(url, error_bad_lines=False)\n",
        "print(shampoo_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TGwsFQ1JyyY"
      },
      "source": [
        "time, values = shampoo_dataset[\"Month\"], shampoo_dataset[\"Sales\"]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(time, values, label = False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4gnkJEZHH-z"
      },
      "source": [
        "#Wspólne cechy szeregów\n",
        "\n",
        "Wiele szeregów czasowych posiada takie właściwości jak:\n",
        "- trend (np. monotoniczny wzrost lub spadek)\n",
        "\n",
        "- sezonowość, którą można zaobserwować jako okres na wykresie (np. ilość turystów w zależności od miesiąca pokazywać będzie największe wartości w okresie letnim)\n",
        "- szum, czyli zakłócenia, drobne błędy wartości występujące w zbiorze danych\n",
        "\n",
        "Poniżej przedstawiono te 3 właściwości na wykresach. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdTP17X3Spkp"
      },
      "source": [
        "#Trend rosnący"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz7g38DcSjPV"
      },
      "source": [
        "def trend(time, slope=0):\n",
        "    return slope * time\n",
        "\n",
        "time = np.arange(4 * 365 + 1)\n",
        "baseline = 10\n",
        "series = baseline + trend(time, 0.1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(time, series)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov-9TAm7Stnk"
      },
      "source": [
        "#Sezonowość"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuTG1g0WBm4c"
      },
      "source": [
        "def seasonal_pattern(season_time):\n",
        "    return np.where(season_time < 0.4,\n",
        "                    np.cos(season_time * 2 * np.pi),\n",
        "                    1 / np.exp(3 * season_time))\n",
        "\n",
        "def seasonality(time, period, amplitude=1, phase=0):\n",
        "    season_time = ((time + phase) % period) / period\n",
        "    return amplitude * seasonal_pattern(season_time)\n",
        "\n",
        "amplitude = 40\n",
        "series = seasonality(time, period=365, amplitude=amplitude)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(time, series)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoEFur5ES4zz"
      },
      "source": [
        "#Szum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9Xa62cRS1UN"
      },
      "source": [
        "def white_noise(time, noise_level=1, seed=None):\n",
        "    rnd = np.random.RandomState(seed)\n",
        "    return rnd.randn(len(time)) * noise_level\n",
        "\n",
        "noise_level = 5\n",
        "noise = white_noise(time, noise_level, seed=42)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(time, noise)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNnUMaQVS-Oj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4d-JEmQTGdT"
      },
      "source": [
        "#Przewidywanie\n",
        "\n",
        "Posiadając zbiór przedstawiający szereg czasowy pewnych wartości możemy dokonać próby oszacowania wartości jakie wystąpią w przyszłości.\n",
        "\n",
        "Użyjemy zbioru przedstawiającego minimalne temperature w Melbourne w latach 1981-1990.\n",
        "\n",
        "Zbiór podzielimy zgodnie z metodą \"Fixed partitioning\" na część do uczenia, walidacyjną i testującą. \n",
        "\n",
        "Wykorzystamy kilka metod - od prostych sztuczek jak naiwne przewidywanie, po bardziej złożone statystyczne metody jak ARIMA, następnie użyjemy sieci neuronowych do przewidywania danych. \n",
        "\n",
        "#Naiwne przewidywanie\n",
        "\n",
        "Najprostszym możliwym sposobem jest uznanie poprzedniej wartości jako przewidywaną. Metoda wydaje się być prymitywna, ale osiągane przez nią rezultaty są warte zaobserwowania chociażby dla celów porównania z innymi metodami.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZfF-wJIVIf4"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/Mervolt/TimeSeriesTutorial/master/melbourne_min_temp.csv\"\n",
        "\n",
        "dataset = pd.read_csv(url)\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REIW-UbzVPZt"
      },
      "source": [
        "split = 3000\n",
        "time, values = dataset[\"Date\"], dataset[\"Temp\"]\n",
        "x_train, y_train = time[:split], values[:split]\n",
        "x_valid, y_valid = time[split:], values[split:] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkRIcHeVWH6r"
      },
      "source": [
        "naive_forecast = values[split - 1:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PiHH9BmWLOz"
      },
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(x_valid, y_valid, label=\"Values\")\n",
        "plot_series(x_valid, naive_forecast, label=\"Forecast\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIobBAwyWgPM"
      },
      "source": [
        "Wykresy nachodzą na siebie w takim stopniu, że nie można ich od siebie odróżnić. Wydzielimy teraz dla celów demonstracyjnych podzbiór."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkDMn8WLWqgj"
      },
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(x_valid, y_valid, start=0, end=150, label=\"Values\")\n",
        "plot_series(x_valid, naive_forecast, start=1, end=151, label=\"Forecast\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw_ZaKp7W9CD"
      },
      "source": [
        "Możemy zaobserwować, że przewidywania są po prostu 1 krok za rzeczywistymi wartościami.\n",
        "\n",
        "W celu ewaluacji potrzebujemy metryk. Użyjemy w tym przypadku metryk średniokwadratowej oraz odległości w przestrzeni Euklidesowej."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW_s6z4FXYEw"
      },
      "source": [
        "print_error(y_valid, naive_forecast)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d70EA78STKYR"
      },
      "source": [
        "#Ruchoma średnia\n",
        "Podejściem, które teraz zostanie zaprezentowane to ruchoma średnia. Jest ono relatywnie proste i polega na wyciąganiu średniej z okresu o długości n. To \"ruchome okno\" o długości n przesuwamy po całym zbiorze danych.\n",
        "Przykładowo dla okna o długości 3, wartość pola o indeksie 7 liczymy jako średnią z pól o indeksach 4, 5 i 6, a dla pola o indeksie 22 z pól o indeksach 19, 20 i 21.\n",
        "\n",
        "Zalety:\n",
        "- redukuje szum\n",
        "\n",
        "Wady:\n",
        "- nie uwzględnia sezonowości\n",
        "- nie uwzględnia trendów"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT9KI0-aTL2v"
      },
      "source": [
        "def moving_average_forecast(values, window_size):\n",
        "  forecast = []\n",
        "  for time in range(len(values) - window_size):\n",
        "    forecast.append(values[time:time + window_size].mean())\n",
        "  return np.array(forecast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBUElAeVdNg_"
      },
      "source": [
        "moving_avg = moving_average_forecast(values, 3)[split - 3:]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(x_valid, y_valid, label=\"Values\")\n",
        "plot_series(x_valid, moving_avg, label=\"Ruchoma średnia (3 dni)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg5wUJEbbg12"
      },
      "source": [
        "print_error(y_valid, moving_avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7gI8Ietfyw6"
      },
      "source": [
        "Osiągneliśmy gorsze wyniki niż w przypadku naiwnego podejścia. Może długość aplikowanego okna była za mała?\n",
        "Spróbujmy dla innej wielkości okna."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCASOtNmcXKg"
      },
      "source": [
        "moving_avg = moving_average_forecast(values, 10)[split - 10:]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(x_valid, y_valid, label=\"Values\")\n",
        "plot_series(x_valid, moving_avg, label=\"Ruchoma średnia (10 dni)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F8ObaArgGg_"
      },
      "source": [
        "print_error(y_valid, moving_avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqWNbFrxgIPO"
      },
      "source": [
        "Wciąż gorsze wyniki. Powodem jest tutaj potężna wada tego podejścia, a mianowicie brak uwzględnienia sezonowości, która w przypadku temperatur jest bardzo ważnym czynnikiem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg4r5f20gYi2"
      },
      "source": [
        "#Ulepszona metoda ruchomej średniej\n",
        "Aby ulepszyć metodę ruchomej średniej należy zlikwidować jej wady - brak uwzględniania trendów oraz brak uwzględniania sezonowości.\n",
        "W tym celu należy specjalnie zadaptować nasz zbiór danych. Zamiast korzystać po prostu z naszego zbioru, korzystać będziemy z różnic (wartość - wartość wcześniejsza o pewien czas t, np. 1 rok i różnica = czerwiec 1984 - czerwiec 1983 )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUsB4HekgHuB"
      },
      "source": [
        "diff_values = (values[365:].reset_index() - values[:-365].reset_index())['Temp']\n",
        "diff_time = time[365:]\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(diff_time, diff_values, label=\"Values(t) – Values(t–365)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDqrFaumjQWk"
      },
      "source": [
        "Podzbiór walidacyjny"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ6SReKLhNth"
      },
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(x_valid, diff_values[split - 365:], label=\"Values(t) – Values(t–365)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfT9EhPkjSiP"
      },
      "source": [
        "diff_moving_avg = moving_average_forecast(diff_values, 30)[split - 365 - 30:]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(x_valid, diff_values[split - 365:], label=\"Values(t) – Values(t–365)\")\n",
        "plot_series(x_valid, diff_moving_avg, label=\"Moving Average of Diff\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl-h5dmtjqCW"
      },
      "source": [
        "W porządku. Obliczyliśmy ruchomą średnią, ale nie jest to nasz zbiór danych. W takim razie musimy go odzyskać.\n",
        "Aby to zrobić należy dodać wartości z przeszłości."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl-HIllSjgw8"
      },
      "source": [
        "diff_moving_avg_plus_past = values[split - 365:-365] + diff_moving_avg\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(time, values, label=\"Values\")\n",
        "plot_series(x_valid, diff_moving_avg_plus_past, label=\"Forecasts\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKtvMsIskHca"
      },
      "source": [
        "print_error(y_valid, diff_moving_avg_plus_past)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuYag8qeo356"
      },
      "source": [
        "Otrzymaliśmy jeszcze gorsze wyniki. Spróbujmy zredukować szum w początkowych danych."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-2MYbE0kLxj"
      },
      "source": [
        "diff_moving_avg_plus_smooth_past = moving_average_forecast(values[split - 370:-359], 11) + diff_moving_avg\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(time, values, label=\"Values\")\n",
        "plot_series(x_valid, diff_moving_avg_plus_smooth_past, label=\"Forecasts\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7hKjnlomei4"
      },
      "source": [
        "print_error(y_valid, diff_moving_avg_plus_smooth_past)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlW4G2w4o9UD"
      },
      "source": [
        "Znacznie lepiej, jednakże wciąż otrzymaliśmy gorsze rezultaty niż w przypadku naiwnego przewidywania"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttKwGoWT9TVI"
      },
      "source": [
        "# ARIMA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYw1f-PT9YWu"
      },
      "source": [
        "Innym podejściem, może być zastosowanie modelu ARIMA (Autoregressive Integrated Moving Average). \n",
        "\n",
        "Model ten składa się z trzech elementów: \n",
        "- proces autoregresyjny **AR**\n",
        "- proces średniej ruchomej **MA**\n",
        "- stopień integracji **I** \n",
        "\n",
        "Proces autoregresyjny oznacza, że każda wartość jest liniową kombinacją poprzednich wartości. Dla Modelu AR(p), dla p=1 proces można przedstawić następująco: \n",
        "\n",
        "$  y_t = y_{t-1} + \\epsilon_t  $\n",
        "\n",
        "Gdzie :\n",
        "\n",
        "$ y_t $ - wartość szeregu w chwili t  \n",
        "\n",
        "$ y_{t-1} $ - wartość szeregu w chwili t-1 \n",
        "\n",
        "$ \\epsilon_t $ - składnik losowy, zaburzenie w chwili t \n",
        "\n",
        "W procesie średniej ruchomej zakładamy, że wartość zalerzy od zaburzeń w chwili obecnej i wcześniejszych, np. model MA(p), dla p = 1 :\n",
        "\n",
        "$  y_t = \\epsilon_{t-1} + \\epsilon_{t}  $\n",
        "\n",
        "Gdzie :\n",
        "\n",
        "$ y_t $ - wartość szeregu w chwili t  \n",
        "\n",
        "$ \\epsilon_{t-1} $ -  składnik losowy, zaburzenie w chwili t -1 \n",
        "\n",
        "$ \\epsilon_t $ - składnik losowy, zaburzenie w chwili t \n",
        "\n",
        "Integracja - oznacza sprowadzenie procesu do postaci stacjonarnej, np. wykres może wykazywać trend, a po integracji, będziemy posiadać wartości stacjonarne, tj. takie, które będą mieć stały poziom odchylenia oraz stały poziom średniej. \n",
        "\n",
        "Model ten uwzględnia 3 istotne parametry p d q, które wpływają na przewidywanie danych w zależności od sezonowości, trendu i szumu. Każdy z parametrów przyjmuje wartości dodatnie: 0,1,2... Natomiast wartość parametru równa 0 oznacza ignorowanie go, np. dla danych bez trendu można ustawić wartość parametru d na 0, przez co model ARIMA(a,0,b) będzie równoważny modelowi ARMA(a,b). Niektóre przykłady modelu ARIMA są odpowiednikami innych modeli, np. ARIMA(0,1,0) - to błądzenie losowe, ARIMA(0,0,0) - to szum biały. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHFA7oss9Vsn"
      },
      "source": [
        "p = q = range(0, 3)\n",
        "d = [0,0,0,0]\n",
        "pdq = list(itertools.product(p, d, q))\n",
        "seasonal_pdq = [(x[0], x[1], x[2], 365) for x in list(itertools.product(p, d, q))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X2Hy1sQ9fHe"
      },
      "source": [
        "Najpierw zajmiemy się poszukiwaniem najlepszych parametrów p, d, q - takich aby wartość AIC była najniższa. Wykorzystamy do tego prostą metodę - sprawdzimy kombinacje p i q dla wartości od 0 do 2, oraz dla wartości d równej 0 - dane nie mają trendu, więc nasz model nie uwzględni parametru d (przyjmie on wartość 0). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjd67nP59cgo"
      },
      "source": [
        "min = 25000\n",
        "min_params = None\n",
        "for param in pdq:\n",
        "      mod = sm.tsa.arima.ARIMA(values,order=param, enforce_stationarity=False, enforce_invertibility=False)\n",
        "      results = mod.fit()\n",
        "      if(results.aic < min):\n",
        "        min = results.aic\n",
        "        min_params = param\n",
        "      print(f'ARIMA{param} - AIC:{results.aic}')\n",
        "\n",
        "print(f'MIN: {min}')\n",
        "print(f'Param: {min_params}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwGv8JT69m_3"
      },
      "source": [
        "model = sm.tsa.arima.ARIMA(values,order=min_params)\n",
        "results = model.fit()\n",
        "print(results.summary().tables[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB_ctVoG9pf7"
      },
      "source": [
        "predictions = results.get_prediction(split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHNlUoY69smm"
      },
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(time, values, label=\"Values\")\n",
        "plot_series(x_valid, predictions.predicted_mean, label=\"Forecast\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBRTKQ1T9uzJ"
      },
      "source": [
        "print_error(y_valid, predictions.predicted_mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0zq_A7g-k2w"
      },
      "source": [
        "Jak widać, uzyskaliśmy najlepsze dotychczas wyniki"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afbM4qu-pUbv"
      },
      "source": [
        "#Przewidywanie - Machine Learning\n",
        "Użyjemy metody regresji liniowej, w oparciu o okno zawierające 30 dni. Nasza początkowa sieć składać się będzie z pojedynczej warstwy, bez użycia funkcji aktywacji. Użycie pędu do optymalizacji wyników z reguły zwiększa zbieżność metody. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsaslqBnpX0f"
      },
      "source": [
        "def window_dataset(series, window_size, batch_size=32,\n",
        "                   shuffle_buffer=1000):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
        "    dataset = dataset.shuffle(shuffle_buffer)\n",
        "    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
        "    dataset = dataset.batch(batch_size).prefetch(1)\n",
        "    return dataset\n",
        "\n",
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "window_size = 30\n",
        "train_set = window_dataset(y_train, window_size)\n",
        "valid_set = window_dataset(y_valid, window_size)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "  keras.layers.Dense(1, input_shape=[window_size])\n",
        "])\n",
        "optimizer = keras.optimizers.SGD(lr=1e-5, momentum=0.9)\n",
        "model.compile(loss=keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\", \"mse\"])\n",
        "model.fit(train_set, epochs=150, validation_data=valid_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvEkHTjJreXG"
      },
      "source": [
        "Sukces! Udało nam się przebić naiwne przewidywanie, jednak ARIMA dalej posiada lepsze wyniki.\n",
        "Możemy spróbować poprawić nasze wyniki na kilka sposobów. Po pierwsze, spróbujemy sprawdzić jaka stała ucząca może być dla nas najlepsza, poprzez zwiększanie jej co pewną ilość epok. Dzięki temu zobaczymy kiedy model uczy się najszybciej. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYq5Wzmp2D-f"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "window_size = 30\n",
        "train_set = window_dataset(y_train, window_size)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "  keras.layers.Dense(1, input_shape=[window_size])\n",
        "])\n",
        "\n",
        "lr_schedule = keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-6 * 10**(epoch / 30))\n",
        "optimizer = keras.optimizers.SGD(lr=1e-6, momentum=0.9)\n",
        "model.compile(loss=keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\"])\n",
        "history = model.fit(train_set, epochs=120, callbacks=[lr_schedule])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tICnlx_P2Kf3"
      },
      "source": [
        "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
        "plt.axis([1e-6, 1e-2, 0, 20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx5NF1Ca6yh0"
      },
      "source": [
        "Jak widać na wykresie, spadek błędu był najlepszy dla wartości ok 1e-4. Dla większych wartości błąd zaczął stawać się o wiele większy. Dzięki temu możemy założyć jaka stała będzie najlepsza dla naszego modelu. \n",
        "Następną rzeczą jaką możemy poprawić to dołożenie do modelu specjalnej metody wczesnego zatrzymania, która sprawdza czy błąd zmienił się na lepsze przez ostatnie kilka epok, jeśli nie to uczenie modelu zostaje przerwane wcześniej. Dzięki temu możemy ustawić ilośc epok na znacznie większą, np. 500 a model będzie się uczył dopóki błąd będzie malał. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izJG2aM32Liy"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "window_size = 30\n",
        "train_set = window_dataset(y_train, window_size)\n",
        "valid_set = window_dataset(y_valid, window_size)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "  keras.layers.Dense(1, input_shape=[window_size])\n",
        "])\n",
        "optimizer = keras.optimizers.SGD(lr=1e-4, momentum=0.9)\n",
        "model.compile(loss=keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\", \"mse\"])\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=10)\n",
        "model.fit(train_set, epochs=500,\n",
        "          validation_data=valid_set,\n",
        "          callbacks=[early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaTfDqU29Dpr"
      },
      "source": [
        "Wynik uległ nieznacznej poprawie. Następnie zdefiniujemy funkcję, która pozwoli nam stworzyć przewidywania dla danych na podstawie modelu. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnlBJnsHrjQo"
      },
      "source": [
        "def model_forecast(model, series, window_size):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "    ds = ds.batch(32).prefetch(1)\n",
        "    forecast = model.predict(ds)\n",
        "    return forecast"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1AJZalyr5dk"
      },
      "source": [
        "lin_forecast = model_forecast(model, values[split - window_size:-1], window_size)[:, 0]\n",
        "lin_forecast.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6xGZf3ar_Ua"
      },
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(time, values, label=\"Values\")\n",
        "plot_series(x_valid, lin_forecast, label=\"Forecast\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO5uXJrtsD30"
      },
      "source": [
        "print_error(y_valid, lin_forecast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meEVAu3UsYRV"
      },
      "source": [
        "Rezultat z dość znaczną poprawą. Spróbujmy jednak poprawić naszą sieć, poprzez dodanie kolejnych wartsw, wykorzystamy w tym celu warstwy Dense oraz dodamy do nich funkcję aktywacji relu. Na początku spróbujemy sprawdzić jaka stała ucząca najlepiej sprawdzi się dla naszych danych. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9M-8HpDqhh8"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "window_size = 30\n",
        "train_set = window_dataset(y_train, window_size)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "  keras.layers.Dense(10, activation=\"relu\", input_shape=[window_size]),\n",
        "  keras.layers.Dense(10, activation=\"relu\"),\n",
        "  keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "lr_schedule = keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-7 * 10**(epoch / 20))\n",
        "optimizer = keras.optimizers.SGD(lr=1e-7, momentum=0.9)\n",
        "model.compile(loss=keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\", \"mse\"])\n",
        "history = model.fit(train_set, epochs=120, callbacks=[lr_schedule])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yai2QZib2Wnc"
      },
      "source": [
        "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
        "plt.axis([1e-7, 1e-2, 0, 30])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7KjeqdJ2ZM5"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "window_size = 30\n",
        "train_set = window_dataset(y_train, window_size)\n",
        "valid_set = window_dataset(y_valid, window_size)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "  keras.layers.Dense(10, activation=\"relu\", input_shape=[window_size]),\n",
        "  keras.layers.Dense(10, activation=\"relu\"),\n",
        "  keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "optimizer = keras.optimizers.SGD(lr=1e-4, momentum=0.9)\n",
        "model.compile(loss=keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\", \"mse\"])\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=15)\n",
        "model.fit(train_set, epochs=500,\n",
        "          validation_data=valid_set,\n",
        "          callbacks=[early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRvrbtA8sgEB"
      },
      "source": [
        "dense_forecast = model_forecast(\n",
        "    model,\n",
        "    values[split - window_size:-1],\n",
        "    window_size)[:, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9_LfmrNtM3T"
      },
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(time, values, label=\"Values\")\n",
        "plot_series(x_valid, dense_forecast, label=\"Forecast\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpUD0hm5tV9u"
      },
      "source": [
        "print_error(y_valid, dense_forecast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4z7o_ij-7j3"
      },
      "source": [
        "Powinniśmy zaobserwować kolejną poprawę wyników. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh-835UCtig0"
      },
      "source": [
        "#RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DppNPs5wtjjJ"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "window_size = 30\n",
        "train_set = window_dataset(y_train, window_size, batch_size=128)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "  keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
        "                      input_shape=[None]),\n",
        "  keras.layers.SimpleRNN(100, return_sequences=True),\n",
        "  keras.layers.SimpleRNN(100),\n",
        "  keras.layers.Dense(1),\n",
        "  keras.layers.Lambda(lambda x: x * 200.0)\n",
        "])\n",
        "lr_schedule = keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-7 * 10**(epoch / 20))\n",
        "optimizer = keras.optimizers.SGD(lr=1e-7, momentum=0.9)\n",
        "model.compile(loss=keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\", \"mse\"])\n",
        "history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7JgcUEJtqMQ"
      },
      "source": [
        "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
        "plt.axis([1e-7, 1e-4, 0, 30])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTgcw4eyJsUm"
      },
      "source": [
        "Zmienimy kształt okna zbioru danych, który zostanie użyty do uczenia sieci, tak aby każda z warstw RNN przekazywała do następnej warstwy wszystkie dane, co powinno przyspieszyc proces uczenia. \n",
        "\n",
        "Uwaga. Ten model może uczyć się dłużej - możesz spróbować zmniejszyć parametry, takie jak ilość epok, lub parametr callbacku Early Stopping, lecz może to skutkować mniejszą dokładnością modelu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdYWD50uyaFT"
      },
      "source": [
        "def seq2seq_window_dataset(series, window_size, batch_size=32,\n",
        "                           shuffle_buffer=1000):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgAaAQDZDG4d"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "window_size = 30\n",
        "train_set = seq2seq_window_dataset(y_train, window_size,\n",
        "                                   batch_size=128)\n",
        "valid_set = seq2seq_window_dataset(y_valid, window_size,\n",
        "                                   batch_size=128)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "  keras.layers.SimpleRNN(100, return_sequences=True,\n",
        "                         input_shape=[None, 1]),\n",
        "  keras.layers.SimpleRNN(100, return_sequences=True),\n",
        "  keras.layers.Dense(1),\n",
        "  keras.layers.Lambda(lambda x: x * 200.0)\n",
        "])\n",
        "optimizer = keras.optimizers.SGD(lr=1e-6, momentum=0.9)\n",
        "model.compile(loss=keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\", \"mse\"])\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=15)\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "    \"my_checkpoint\", save_best_only=True)\n",
        "model.fit(train_set, epochs=500,\n",
        "          validation_data=valid_set,\n",
        "          callbacks=[early_stopping, model_checkpoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3KnM-BiDQdh"
      },
      "source": [
        "rnn_forecast = model_forecast(model,values[..., np.newaxis], window_size)[split - window_size:-1,-1,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiRkJHv7DZlE"
      },
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(time, values, label=\"Values\")\n",
        "plot_series(x_valid, rnn_forecast, label=\"Forecast\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UJguOkwDcZQ"
      },
      "source": [
        "print_error(y_valid, rnn_forecast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulVLXnSjo4Tf"
      },
      "source": [
        "#LSTRM RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41aEQP9Co5nF"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "window_size = 30\n",
        "train_set = sequential_window_dataset(y_train, window_size)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "  keras.layers.LSTM(100, return_sequences=True, stateful=True,\n",
        "                    batch_input_shape=[1, None, 1]),\n",
        "  keras.layers.LSTM(100, return_sequences=True, stateful=True),\n",
        "  keras.layers.Dense(1),\n",
        "  keras.layers.Lambda(lambda x: x * 200.0)\n",
        "])\n",
        "lr_schedule = keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
        "reset_states = ResetStatesCallback()\n",
        "optimizer = keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
        "model.compile(loss=keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\", \"mse\"])\n",
        "history = model.fit(train_set, epochs=100,\n",
        "                    callbacks=[lr_schedule, reset_states])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhbvkGCfpuu6"
      },
      "source": [
        "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
        "plt.axis([1e-8, 1e-4, 0, 30])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ1V7L0Xpw2e"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "window_size = 30\n",
        "train_set = sequential_window_dataset(y_train, window_size)\n",
        "valid_set = sequential_window_dataset(y_valid, window_size)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "  keras.layers.LSTM(100, return_sequences=True, stateful=True,\n",
        "                         batch_input_shape=[1, None, 1]),\n",
        "  keras.layers.LSTM(100, return_sequences=True, stateful=True),\n",
        "  keras.layers.Dense(1),\n",
        "  keras.layers.Lambda(lambda x: x * 200.0)\n",
        "])\n",
        "optimizer = keras.optimizers.SGD(lr=5e-7, momentum=0.9)\n",
        "model.compile(loss=keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\"])\n",
        "reset_states = ResetStatesCallback()\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "    \"my_checkpoint.h5\", save_best_only=True)\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=15)\n",
        "model.fit(train_set, epochs=500,\n",
        "          validation_data=valid_set,\n",
        "          callbacks=[early_stopping, model_checkpoint, reset_states])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qmvTwVepzCM"
      },
      "source": [
        "model = keras.models.load_model(\"my_checkpoint.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwNfGgaUpzpV"
      },
      "source": [
        "rnn_forecast = model.predict(values[np.newaxis, :, np.newaxis])\n",
        "rnn_forecast = rnn_forecast[0, split - 1:-1, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fetVDX-9p0vV"
      },
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(time, values)\n",
        "plot_series(x_valid, rnn_forecast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP2Yehuep2xi"
      },
      "source": [
        "print_error(y_valid, rnn_forecast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZXjrQZs2DHl"
      },
      "source": [
        "#CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cVlU44q2J0X"
      },
      "source": [
        "Ostatnia sieć dokłada do warstw LSTM, warstwę konwolucyjną. Na początku  zdefiniujemy odpowiednią stałą uczącą w sposób analogiczny jak dotychczas. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B898NyrN2AAn"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "window_size = 30\n",
        "train_set = seq2seq_window_dataset(y_train, window_size,\n",
        "                                   batch_size=128)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "  keras.layers.Conv1D(filters=32, kernel_size=5,\n",
        "                      strides=1, padding=\"causal\",\n",
        "                      activation=\"relu\",\n",
        "                      input_shape=[None, 1]),\n",
        "  keras.layers.LSTM(32, return_sequences=True),\n",
        "  keras.layers.LSTM(32, return_sequences=True),\n",
        "  keras.layers.Dense(1),\n",
        "  keras.layers.Lambda(lambda x: x * 200)\n",
        "])\n",
        "lr_schedule = keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
        "optimizer = keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
        "model.compile(loss=keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\", \"mse\"])\n",
        "history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8EPm3cs3Vf-"
      },
      "source": [
        "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
        "plt.axis([1e-8, 1e-4, 0, 30])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHE3GYKp3YYH"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "window_size = 30\n",
        "train_set = seq2seq_window_dataset(y_train, window_size,\n",
        "                                   batch_size=128)\n",
        "valid_set = seq2seq_window_dataset(y_valid, window_size,\n",
        "                                   batch_size=128)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "  keras.layers.Conv1D(filters=32, kernel_size=5,\n",
        "                      strides=1, padding=\"causal\",\n",
        "                      activation=\"relu\",\n",
        "                      input_shape=[None, 1]),\n",
        "  keras.layers.LSTM(32, return_sequences=True),\n",
        "  keras.layers.LSTM(32, return_sequences=True),\n",
        "  keras.layers.Dense(1),\n",
        "  keras.layers.Lambda(lambda x: x * 200)\n",
        "])\n",
        "optimizer = keras.optimizers.SGD(lr=1e-5, momentum=0.9)\n",
        "model.compile(loss=keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\", \"mse\"])\n",
        "\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "    \"my_checkpoint_cnn.h5\", save_best_only=True)\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=15)\n",
        "model.fit(train_set, epochs=500,\n",
        "          validation_data=valid_set,\n",
        "          callbacks=[early_stopping, model_checkpoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAuRJwkM3oQq"
      },
      "source": [
        "model = keras.models.load_model(\"my_checkpoint_cnn.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_s7XOAS3qjj"
      },
      "source": [
        "cnn_forecast = model_forecast(model, values[..., np.newaxis], window_size)\n",
        "cnn_forecast = cnn_forecast[split - window_size:-1, -1, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1czr7TCc3rDe"
      },
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(time, values)\n",
        "plot_series(x_valid, cnn_forecast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMf-XKDQ32xo"
      },
      "source": [
        "print_error(y_valid, cnn_forecast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtD8c7oLB2-b"
      },
      "source": [
        "Osiągane wyniki są najlepsze spośród wszystkich modeli opartych na sieciach neuronowych, jednak metoda ARIMA osiąga ogółem najlepszy wynik dla użytych danych."
      ]
    }
  ]
}